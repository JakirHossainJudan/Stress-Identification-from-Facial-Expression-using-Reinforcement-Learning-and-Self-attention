# -*- coding: utf-8 -*-
"""Real-Time Stress Identification from Facial  Expression using Reinforcement Learning and  Self-attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13rqCictJQNsx5ST7fAli3j3gIMeVl7yV
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, datasets
import numpy as np
import os
from PIL import Image
import random

import zipfile
import os

zip_path = '/content/drive/MyDrive/Dataset/Facial Emotion Detection.v1i.yolov11.zip'
extract_path = '/content/emotion_dataset'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
data_path = '/content/emotion_dataset'  # Adjust this path
image_size = 64
num_classes = 7  # Change based on your dataset
batch_size = 32
epochs = 5
ppo_epochs = 4
clip_param = 0.2
learning_rate = 3e-4
gamma = 0.99

transform = transforms.Compose([
    transforms.Resize((image_size, image_size)),
    transforms.ToTensor(),
])

dataset = datasets.ImageFolder(data_path, transform=transform)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

class SelfAttention(nn.Module):
    def __init__(self, in_dim):
        super(SelfAttention, self).__init__()
        self.query = nn.Conv2d(in_dim, in_dim // 8, 1)
        self.key = nn.Conv2d(in_dim, in_dim // 8, 1)
        self.value = nn.Conv2d(in_dim, in_dim, 1)
        self.gamma = nn.Parameter(torch.zeros(1))

def forward(self, x):
        B, C, H, W = x.size()
        proj_query = self.query(x).view(B, -1, H * W).permute(0, 2, 1)
        proj_key = self.key(x).view(B, -1, H * W)
        attention = torch.bmm(proj_query, proj_key)
        attention = torch.softmax(attention, dim=-1)
        proj_value = self.value(x).view(B, -1, H * W)

        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(B, C, H, W)
        out = self.gamma * out + x
        return out

class EmotionPolicy(nn.Module):
    def __init__(self, num_classes):
        super(EmotionPolicy, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.ReLU(),
            SelfAttention(128),
            nn.Conv2d(128, 256, 4, 2, 1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes),
            nn.Softmax(dim=-1)
        )

def forward(self, x):
        features = self.features(x)
        return self.classifier(features)

def ppo_loss(old_probs, new_probs, actions, rewards):
    advantage = rewards
    ratio = torch.exp(torch.log(new_probs + 1e-10) - torch.log(old_probs + 1e-10))
    surr1 = ratio * advantage
    surr2 = torch.clamp(ratio, 1 - clip_param, 1 + clip_param) * advantage
    return -torch.min(surr1, surr2).mean()

model = EmotionPolicy(num_classes).to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

class EmotionPolicy(nn.Module):
    def __init__(self, num_classes):
        super(EmotionPolicy, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.ReLU(),
            SelfAttention(128),
            nn.Conv2d(128, 256, 4, 2, 1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes),
            nn.Softmax(dim=-1)
        )

def forward(self, x):
    features = self.features(x)
    return self.classifier(features)

import torch.nn as nn

# Self-Attention Block
class SelfAttention(nn.Module):
    def __init__(self, in_dim):
        super(SelfAttention, self).__init__()
        self.query = nn.Conv2d(in_dim, in_dim // 8, 1)
        self.key = nn.Conv2d(in_dim, in_dim // 8, 1)
        self.value = nn.Conv2d(in_dim, in_dim, 1)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        B, C, H, W = x.size()
        proj_query = self.query(x).view(B, -1, H * W).permute(0, 2, 1)
        proj_key = self.key(x).view(B, -1, H * W)
        attention = torch.bmm(proj_query, proj_key)
        attention = torch.softmax(attention, dim=-1)
        proj_value = self.value(x).view(B, -1, H * W)

        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(B, C, H, W)
        out = self.gamma * out + x
        return out

# FIXED EmotionPolicy model with forward()
class EmotionPolicy(nn.Module):
    def __init__(self, num_classes):
        super(EmotionPolicy, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.ReLU(),
            SelfAttention(128),
            nn.Conv2d(128, 256, 4, 2, 1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes),
            nn.Softmax(dim=-1)
        )

    def forward(self, x):  # ðŸ‘ˆ This is crucial!
        features = self.features(x)
        return self.classifier(features)

model = EmotionPolicy(num_classes=7).to(device)

# ========== PPO LOSS FUNCTION ========== #
def ppo_loss(old_probs, new_probs, actions, rewards):
    advantage = rewards
    ratio = torch.exp(torch.log(new_probs + 1e-10) - torch.log(old_probs + 1e-10))
    surr1 = ratio * advantage
    surr2 = torch.clamp(ratio, 1 - clip_param, 1 + clip_param) * advantage
    return -torch.min(surr1, surr2).mean()

# ========== TRAINING LOOP ========== #
model = EmotionPolicy(num_classes).to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(epochs):
    for images, labels in dataloader:
        images, labels = images.to(device), labels.to(device)

        with torch.no_grad():
            old_probs = model(images).detach()

        for _ in range(ppo_epochs):
            logits = model(images)
            probs = torch.gather(logits, 1, labels.view(-1, 1)).squeeze()

            rewards = torch.ones_like(probs)  # Reward = 1 for correct prediction (or build a more complex one)
            loss = ppo_loss(old_probs.gather(1, labels.view(-1, 1)).squeeze(), probs, labels, rewards)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")

"""# **Correlation HeatMap**"""

!pip install seaborn

import torch
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np

# Set model to eval mode
model.eval()

# Collect predictions and true labels
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in dataloader:
        images = images.to(device)
        outputs = model(images)
        preds = torch.argmax(outputs, dim=1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

# Generate confusion matrix
cm = confusion_matrix(all_labels, all_preds)
cm_normalized = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]

from sklearn.metrics import confusion_matrix
import numpy as np

# Create confusion matrix (non-normalized)
cm = confusion_matrix(all_labels, all_preds, labels=range(len(dataset.classes)))

# Normalize row-wise (true labels)
cm_normalized = cm.astype("float") / cm.sum(axis=1, keepdims=True)

# Handle divide-by-zero (if any class has 0 true samples)
cm_normalized = np.nan_to_num(cm_normalized)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))
sns.heatmap(cm_normalized, annot=True, fmt=".2f", cmap="Blues",
            xticklabels=dataset.classes,
            yticklabels=dataset.classes)

plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Normalized Confusion Matrix - Emotion Recognition")
plt.show()

"""# **Deep Feature Correlation Heatmap**"""

import torch
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader

# Modify the model slightly to expose the feature embeddings
class EmotionFeatureExtractor(EmotionPolicy):
    def __init__(self, num_classes):
        super().__init__(num_classes)

    def extract_features(self, x):
        with torch.no_grad():
            features = self.features(x)
            features = features.view(features.size(0), -1)  # Flatten
        return features

# Modify the model slightly to expose the feature embeddings
class EmotionFeatureExtractor(EmotionPolicy):
    def __init__(self, num_classes):
        super().__init__(num_classes)

    def extract_features(self, x):
        with torch.no_grad():
            features = self.features(x)
            features = features.view(features.size(0), -1)  # Flatten
        return features

# Collect all features and labels
all_features = []

# Extend your EmotionPolicy class to include a feature extractor
class EmotionFeatureExtractor(EmotionPolicy):
    def __init__(self, num_classes):
        super().__init__(num_classes)

    def extract_features(self, x):
        with torch.no_grad():
            features = self.features(x)
            features = features.view(features.size(0), -1)  # Flatten
        return features

# Create the feature model and copy the trained weights
feature_model = EmotionFeatureExtractor(num_classes=7).to(device)
feature_model.load_state_dict(model.state_dict())  # <-- Use your trained model
feature_model.eval()

all_features = []

with torch.no_grad():
    for images, _ in dataloader:
        images = images.to(device)
        feats = feature_model.extract_features(images)
        all_features.append(feats.cpu().numpy())

# Stack into full feature matrix [num_samples x feature_dim]
feature_matrix = np.vstack(all_features)  # Shape: [N x D]

# Compute correlation between feature dimensions (D x D)
correlation_matrix = np.corrcoef(feature_matrix.T)

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, cmap='Blues', center=0)  # Changed cmap to 'Blues'
plt.title("Correlation Heatmap of Deep Features (Neurons)")
plt.xlabel("Feature Dimension")
plt.ylabel("Feature Dimension")
plt.show()

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Assuming `correlation_matrix` is already computed from your deep features
plt.figure(figsize=(20, 16))  # Increase figure size

sns.set(font_scale=1.2)  # Scale up font for readability
sns.heatmap(
    correlation_matrix,
    cmap='Blues',
    center=0,
    square=True,
    linewidths=0.5,
    cbar_kws={"shrink": 0.8},
)

plt.title("Deep Feature Correlation Heatmap", fontsize=18)
plt.xlabel("Feature Dimension", fontsize=14)
plt.ylabel("Feature Dimension", fontsize=14)
plt.tight_layout()
plt.show()

"""# **Accuracy Calculation**"""

from sklearn.metrics import accuracy_score

# Set model to evaluation mode
model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in dataloader:
        images = images.to(device)
        labels = labels.to(device)

        outputs = model(images)
        preds = torch.argmax(outputs, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calculate accuracy
accuracy = accuracy_score(all_labels, all_preds)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

"""# **Full Code: F1, Precision, Recall, P-values in Table**"""

import pandas as pd
from sklearn.metrics import precision_recall_fscore_support
from scipy.stats import chi2_contingency

# Calculate precision, recall, f1 for each class
precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, labels=range(len(dataset.classes)))

# Calculate overall macro averages
avg_precision = precision.mean()
avg_recall = recall.mean()
avg_f1 = f1.mean()

from scipy.stats import chi2_contingency
import numpy as np

# P-values: chi-squared test for each class vs the rest
p_values = []
epsilon = 1e-6  # Small value to avoid zero frequencies

for i in range(len(dataset.classes)):
    # Build contingency table
    tp = sum((np.array(all_preds) == i) & (np.array(all_labels) == i))
    fp = sum((np.array(all_preds) == i) & (np.array(all_labels) != i))
    fn = sum((np.array(all_preds) != i) & (np.array(all_labels) == i))
    tn = sum((np.array(all_preds) != i) & (np.array(all_labels) != i))

    # Apply smoothing to avoid zero frequencies
    contingency = np.array([
        [tp + epsilon, fp + epsilon],
        [fn + epsilon, tn + epsilon]
    ])

    try:
        _, p, _, _ = chi2_contingency(contingency)
    except ValueError:
        p = 1.0  # fallback p-value if test fails

    p_values.append(p)

# Create DataFrame
df = pd.DataFrame({
    'Average': [avg_f1, avg_precision, avg_recall, np.mean(p_values)],
    **{cls: [f1[i], precision[i], recall[i], p_values[i]] for i, cls in enumerate(dataset.classes)}
}, index=['F1 Score', 'Precision', 'Recall', 'P-Value'])

# Move 'Average' to the first column
df = df[['Average'] + [cls for cls in dataset.classes]]

# Display the table
print("ðŸ“Š Emotion Classification Report")
display(df.style.background_gradient(cmap='Blues').format("{:.4f}"))

"""# **Micro/macro/weighted averages or a plot of the scores too!**"""

from sklearn.metrics import classification_report

# Generate classification report as dict
report_dict = classification_report(
    all_labels,
    all_preds,
    target_names=dataset.classes,
    output_dict=True,
    zero_division=0
)

# Extract the values into a DataFrame
score_df = pd.DataFrame(report_dict).T[['precision', 'recall', 'f1-score']]

# Rename index for averages
score_df.rename(index={
    'macro avg': 'Macro Avg',
    'weighted avg': 'Weighted Avg',
    'micro avg': 'Micro Avg'
}, inplace=True)

# Only use the ones that exist
available_avgs = [avg for avg in ['Macro Avg', 'Weighted Avg', 'Micro Avg'] if avg in score_df.index]

# Move existing averages to top
averages = score_df.loc[available_avgs]
per_class = score_df.drop(available_avgs, errors='ignore')
final_df = pd.concat([averages, per_class])

# Add p-values to per-class section (not available for averages)
pval_series = pd.Series(p_values, index=dataset.classes)
final_df['P-Value'] = final_df.index.map(pval_series).fillna('-')

# Identify float columns only
float_cols = final_df.select_dtypes(include='float').columns

# Apply formatting only to those
display(final_df.style
        .background_gradient(cmap='Blues')
        .format({col: "{:.4f}" for col in float_cols}))

"""# **Plot F1/Precision/Recall per class**"""

import matplotlib.pyplot as plt

# Plot per-class metrics (excluding averages)
metrics = ['precision', 'recall', 'f1-score']
classes = [cls for cls in dataset.classes]

plt.figure(figsize=(10, 6))
for metric in metrics:
    plt.plot(classes, final_df.loc[classes][metric], marker='o', label=metric.title())

plt.title('Emotion Classification Metrics per Class')
plt.xlabel('Emotion Class')
plt.ylabel('Score')
plt.ylim(0, 1.05)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""## **Comparing accuracy and F1-scores per emotion class, possibly across multiple models or settings**"""

from sklearn.metrics import accuracy_score, f1_score
import pandas as pd
import numpy as np

# Unique classes
label_names = dataset.classes  # or manually: ['Happy', 'Sad', 'Neutral', ...]
n_classes = len(label_names)

# Per-class accuracy and F1
accs, f1s = [], []
for i in range(n_classes):
    true_class = np.array(all_labels) == i
    pred_class = np.array(all_preds) == i

    acc = accuracy_score(true_class, pred_class)
    f1 = f1_score(true_class, pred_class, zero_division=0)

    accs.append(round(acc * 100, 2))
    f1s.append(round(f1 * 100, 2))

# Average
avg_acc = round(np.mean(accs), 2)
avg_f1 = round(np.mean(f1s), 2)

# Create DataFrame
columns = []
data = []

# Create DataFrame
columns = []
data = []

for emotion, acc, f1 in zip(label_names, accs, f1s):
    columns.extend([f"{emotion} Acc.", f"{emotion} F1"])
    data.extend([acc, f1])

columns.extend(["Average Acc.", "Average F1"])
data.extend([avg_acc, avg_f1])

table_df = pd.DataFrame([data], columns=columns, index=["PPO-SelfAtt"])

display(table_df.style
        .set_caption("Emotion-wise Accuracy and F1 Scores")
        .format("{:.2f}")
        .background_gradient(cmap="Blues", axis=1))

"""# **Same as The Paper**"""

from sklearn.metrics import accuracy_score, f1_score
import pandas as pd
import numpy as np

# Example emotion classes (change if your dataset is different)
emotions = dataset.classes  # or manually: ['Happy', 'Sad', 'Neutral', 'Angry', 'Excited', 'Frustrated']

# Collect per-class scores
accs, f1s = [], []
for i in range(len(emotions)):
    true_class = (np.array(all_labels) == i)
    pred_class = (np.array(all_preds) == i)

    acc = accuracy_score(true_class, pred_class)
    f1 = f1_score(true_class, pred_class, zero_division=0)

    accs.append(round(acc * 100, 2))
    f1s.append(round(f1 * 100, 2))

# Averages
avg_acc = round(np.mean(accs), 2)
avg_f1 = round(np.mean(f1s), 2)

# Create multi-index columns like in the paper table
columns = pd.MultiIndex.from_tuples(
    [(emotion, 'Acc.') for emotion in emotions] +
    [(emotion, 'F1') for emotion in emotions] +
    [('Average', 'Acc.')] + [('Average', 'F1')],
    names=['Emotion', 'Metric']
)

# Row values
values = accs + f1s + [avg_acc, avg_f1]

# Final DataFrame
table_df = pd.DataFrame([values], columns=columns, index=['PPO-SelfAtt'])

# Display styled like paper
display(table_df.style.set_caption("Emotion Recognition Performance")
        .format("{:.2f}")
        .background_gradient(cmap='Blues', axis=1))

print(table_df.to_latex(multicolumn=True, multirow=True))

"""# **OUTPUT**"""

!pip install fer

from IPython.display import display, Javascript
from google.colab.output import eval_js
import cv2
import PIL.Image
import io
import base64
import numpy as np
import time

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});
      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getTracks().forEach(t => t.stop());
      div.remove();

      return canvas.toDataURL('image/jpeg', quality);
    }
    takePhoto({quality: %f});
  ''' % quality)

  display(js)
  data = eval_js("takePhoto({quality: %f})" % quality)
  binary = io.BytesIO(base64.b64decode(data.split(',')[1]))
  image = PIL.Image.open(binary)
  image.save(filename)
  return filename

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

from fer import FER
detector = FER()

from fer import FER
import matplotlib.pyplot as plt

detector = FER()

# Run detection loop
num_frames = 10  # adjust how many frames you want
all_results = []

for i in range(num_frames):
    print(f"\nðŸ“¸ Capturing frame {i+1}/{num_frames}...")
    filename = take_photo(f'frame_{i+1}.jpg')

    # Read and convert image
    img = cv2.imread(filename)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Detect emotion
    result = detector.detect_emotions(img_rgb)
    plt.imshow(img_rgb)
    plt.axis('off')
    plt.title(f"Frame {i+1}")
    plt.show()

    # Show emotions
    if result:
        emotions = result[0]["emotions"]
        print("Detected Emotions:")
        for emotion, score in emotions.items():
            print(f"{emotion:<10}: {score:.2f}")
        all_results.append(emotions)
    else:
        print("No face detected.")

    time.sleep(1)  # short delay between frames

# Summary after loop
print("\nâœ… Done capturing all frames.")